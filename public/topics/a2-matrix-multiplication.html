<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A2: Matrix Multiplication — Neural Pathways</title>
  <meta name="description" content="Animate row-by-column multiplication step-by-step. See matrices transforming space through rotation, scaling, and shearing.">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <link rel="stylesheet" href="../css/style.css">

  <style>
    /* * Custom Layout Overrides for "Immersive Mode"
     * This creates the sticky visualization side-by-side layout
     */

    /* Base setup for the split layout container */
    .split-layout-container {
      width: 100%;
      max-width: 100%;
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    /* Default mobile/tablet view (Stacked) */
    .viz-panel {
      width: 100%;
      padding: 1rem;
    }
    
    .content-panel {
      width: 100%;
      padding: 1rem;
      max-width: 800px; /* Readable line length on mobile */
      margin: 0 auto;
    }

    /* The aspect ratio box */
    .viz-aspect-box {
      width: 100%;
      position: relative;
      /* Enforce 4:3 Aspect Ratio */
      aspect-ratio: 4 / 3; 
      background: #0f1115; /* Fallback dark bg */
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 10px 30px rgba(0,0,0,0.5);
      border: 1px solid rgba(255,255,255,0.1);
    }

    .viz-aspect-box iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: none;
    }

    /* Floating Home Button Styles */
    .back-home {
      position: fixed;
      bottom: 24px;
      right: 24px;
      width: 56px;
      height: 56px;
      background-color: #1a1d21; /* Dark surface color to match theme */
      color: #e2e8f0;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 20px rgba(0,0,0,0.4);
      z-index: 100;
      transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275);
      text-decoration: none; /* Remove underline */
    }
    
    .back-home:hover {
      background-color: #3b82f6; /* Primary blue accent */
      color: white;
      transform: translateY(-4px) scale(1.05);
      box-shadow: 0 8px 25px rgba(59, 130, 246, 0.4);
      border-color: transparent;
    }

    .back-home svg {
      width: 24px;
      height: 24px;
    }

    /* DESKTOP LAYOUT (Split Screen) */
    @media (min-width: 1024px) {
      .split-layout-container {
        display: flex;
        flex-direction: row;
        align-items: flex-start; /* Important for sticky to work */
        min-height: 100vh;
      }

      /* Left Side: Visualization (Sticky) */
      .viz-panel {
        /* Make it big: 60% of width */
        width: 60%;
        /* Sticky Magic */
        position: sticky;
        top: 0;
        height: 100vh; /* Fill vertical height of viewport */
        display: flex;
        align-items: center; /* Center viz vertically */
        justify-content: center;
        padding: 2rem;
        background: #050505; /* Slightly darker bg for viz area */
        border-right: 1px solid rgba(255,255,255,0.05);
        box-sizing: border-box;
        z-index: 10;
      }

      .viz-aspect-box {
        width: 100%; 
        /* The aspect ratio is handled by the class definition above */
      }

      /* Right Side: Content (Scrollable) */
      .content-panel {
        width: 40%;
        padding: 3rem;
        box-sizing: border-box;
        /* Add some bottom padding so content isn't hidden behind the fixed button */
        padding-bottom: 6rem;
      }

      /* Adjust breadcrumb/header spacing for this tight layout */
      .topic-header {
        margin-top: 1rem;
      }
    }

    /* Large desktop tweaks */
    @media (min-width: 1600px) {
      .viz-panel { width: 65%; }
      .content-panel { width: 35%; }
    }
  </style>
</head>
<body>
  <header class="header">
    <div class="container">
      <a href="../index.html" class="logo">
        <div class="logo-icon">
          <svg viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg">
            <circle cx="12" cy="12" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="12" cy="36" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="24" cy="18" r="4" fill="currentColor"/>
            <circle cx="24" cy="30" r="4" fill="currentColor"/>
            <circle cx="36" cy="24" r="5" fill="currentColor"/>
            <line x1="16" y1="12" x2="20" y2="18" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="16" y1="36" x2="20" y2="30" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="18" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="30" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
          </svg>
        </div>
        <div class="logo-text">
          <h1>Neural Pathways</h1>
          <span>Interactive ML Visualizations</span>
        </div>
      </a>
    </div>
  </header>

  <main class="topic-page">
    <div class="split-layout-container">
      
      <!-- 1. Visualization Panel (Sticky on Desktop) -->
      <aside class="viz-panel">
        <div class="viz-aspect-box">
          <iframe src="../../viz/A2-Matrix-Multiplication.html" title="Interactive Matrix Multiplication Visualizer" loading="lazy"></iframe>
        </div>
      </aside>

      <!-- 2. Content Panel (Scrollable) -->
      <div class="content-panel">
        
        <!-- Navigation & Header moved inside the scrolling content pane -->
        <nav class="breadcrumb">
          <a href="../index.html">Home</a>
          <span>›</span>
          <a href="../index.html#cat-a">Linear Algebra</a>
          <span>›</span>
          <span>A2</span>
        </nav>

        <header class="topic-header" style="--category-color: var(--cat-a)">
          <div class="topic-meta">
            <span class="topic-badge" style="background: var(--cat-a)">A2</span>
            <span class="topic-category">Linear Algebra Fundamentals</span>
          </div>
          <h1>Matrix Multiplication</h1>
          <p class="topic-description">Animate row-by-column multiplication step-by-step. Show matrices transforming space (rotation, scaling, shearing).</p>
        </header>

        <!-- Educational Content -->
        <div class="content-layout--single">
          <article class="educational-content">
          
          <p>You've mastered vectors—arrows in space, lists of numbers that can be added and scaled. Now it's time to meet their powerful sibling: the <strong>matrix</strong>. If vectors are the nouns of linear algebra, matrices are the verbs. They <em>do</em> things to vectors. Every neural network layer, every 3D graphics transformation, every recommendation system—matrices are doing the heavy lifting.</p>

          <h2>1. What is a Matrix?</h2>
          
          <p>A matrix is just a rectangular grid of numbers. That's it. But like many simple things in mathematics, this humble grid turns out to be extraordinarily useful because we can interpret it in multiple ways.</p>
          
          <p><strong>Think of a matrix as:</strong></p>
          
          <p><strong>A table of numbers</strong> — The most literal interpretation. Just like a spreadsheet with rows and columns:</p>
          
<pre><code class="language-typescript">const matrix = [
  [1, 2, 3],
  [4, 5, 6]
];</code></pre>
          
          <p><strong>A collection of vectors</strong> — You can read a matrix as a stack of row vectors (reading horizontally) or as a set of column vectors standing side by side (reading vertically). This dual interpretation becomes crucial when we multiply.</p>
          
          <p><strong>A transformation function</strong> — Here's where it gets interesting. A matrix can represent a <em>function</em> that takes a vector as input and produces a new vector as output. Feed in a point, get back a transformed point. This is how 3D graphics engines rotate and scale objects, and how neural networks transform data layer by layer.</p>
          
          <p><strong>A lookup/mapping table</strong> — In ML contexts, think of weight matrices as encoding which input features influence which outputs, and by how much.</p>

          <hr>
          
          <h2>2. Matrix Dimensions</h2>
          
          <p>We describe matrix size as <strong>rows × columns</strong> (always rows first). A matrix with 2 rows and 3 columns is a "2×3 matrix" or "2 by 3 matrix."</p>
          
<pre><code class="language-typescript">// A 2×3 matrix: 2 rows, 3 columns
const A: number[][] = [
  [1, 2, 3],    // row 0
  [4, 5, 6]     // row 1
];
// Columns: 0  1  2

// Shape in NumPy/TensorFlow style: (2, 3)</code></pre>
          
          <p><strong>The compatibility rule for multiplication:</strong> To multiply matrix A by matrix B (written AB), the number of <strong>columns in A</strong> must equal the number of <strong>rows in B</strong>.</p>
          
<pre><code>A: (m × n)  ×  B: (n × p)  =  C: (m × p)
         ↑___↑
    These must match!</code></pre>
          
          <p>Think of it like connecting LEGO blocks—the "output width" of the first matrix must match the "input height" of the second. If A is 2×3 and B is 3×4, the result is 2×4. If A is 2×3 and B is 2×4? Error. They don't connect.</p>

          <hr>
          
          <h2>3. The Row-by-Column Algorithm</h2>
          
          <p>Matrix multiplication isn't just multiplying corresponding elements (that's a different operation called element-wise or Hadamard product). Instead, each element in the result comes from a <strong>dot product</strong> between a row of the first matrix and a column of the second.</p>
          
          <p>Let's work through a concrete example: multiplying a 2×3 matrix by a 3×2 matrix.</p>
          
<pre><code class="language-typescript">// A is 2×3
const A = [
  [1, 2, 3],
  [4, 5, 6]
];

// B is 3×2
const B = [
  [7, 8],
  [9, 10],
  [11, 12]
];

// Result C will be 2×2</code></pre>
          
          <p><strong>The algorithm:</strong> For position (i, j) in the result, take row i from A, take column j from B, compute their dot product.</p>
          
<pre><code>C[0][0] = (row 0 of A) · (column 0 of B)
        = [1, 2, 3] · [7, 9, 11]
        = 1×7 + 2×9 + 3×11
        = 7 + 18 + 33 = 58

C[0][1] = (row 0 of A) · (column 1 of B)
        = [1, 2, 3] · [8, 10, 12]
        = 1×8 + 2×10 + 3×12
        = 8 + 20 + 36 = 64

C[1][0] = (row 1 of A) · (column 0 of B)
        = [4, 5, 6] · [7, 9, 11]
        = 4×7 + 5×9 + 6×11
        = 28 + 45 + 66 = 139

C[1][1] = (row 1 of A) · (column 1 of B)
        = [4, 5, 6] · [8, 10, 12]
        = 4×8 + 5×10 + 6×12
        = 32 + 50 + 72 = 154</code></pre>
          
          <p><strong>Result:</strong></p>
<pre><code class="language-typescript">const C = [
  [58, 64],
  [139, 154]
];</code></pre>
          
          <p>Here's what that looks like as code:</p>
          
<pre><code class="language-typescript">function matmul(A: number[][], B: number[][]): number[][] {
  const m = A.length;        // rows in A
  const n = A[0].length;     // cols in A (must equal rows in B)
  const p = B[0].length;     // cols in B
  
  const C: number[][] = [];
  
  for (let i = 0; i < m; i++) {
    C[i] = [];
    for (let j = 0; j < p; j++) {
      let sum = 0;
      for (let k = 0; k < n; k++) {
        sum += A[i][k] * B[k][j];  // row i of A, column j of B
      }
      C[i][j] = sum;
    }
  }
  return C;
}</code></pre>
          
          <p>Notice the three nested loops—this is why matrix multiplication is O(n³) for square matrices. It's computationally expensive, which is why GPUs (massively parallel processors) revolutionised deep learning.</p>

          <hr>
          
          <h2>4. Why Order Matters: AB ≠ BA</h2>
          
          <p>Unlike regular number multiplication (3 × 5 = 5 × 3), matrix multiplication is <strong>not commutative</strong>. Swapping the order gives a different result—or might not even be valid.</p>
          
          <p><strong>Dimension incompatibility:</strong> If A is 2×3 and B is 3×4, then AB exists (result is 2×4), but BA doesn't exist at all (4 columns doesn't match 2 rows).</p>
          
          <p><strong>Different results even when both are valid:</strong> Even when both orders work (like with square matrices), you typically get different answers.</p>
          
          <p><strong>Geometric intuition:</strong> Imagine two transformations—a 90° rotation and a 2× horizontal stretch.</p>
          
          <p>Rotate then stretch: Take a square, rotate it 45° so it's a diamond, then stretch horizontally. You get a wide diamond.</p>
          
          <p>Stretch then rotate: Take a square, stretch it into a wide rectangle, then rotate. You get a tall rectangle at an angle.</p>
          
          <p>Completely different results! The order of transformations matters, and matrix multiplication order reflects this. When you write AB and apply it to a vector v, you're computing A(Bv)—B acts first, then A acts on the result. Reading right to left might feel backwards, but it matches function composition: (f ∘ g)(x) = f(g(x)).</p>

          <hr>
          
          <h2>5. Matrices as Transformations</h2>
          
          <p>This is where matrices become visual and intuitive. A 2×2 matrix transforms 2D space; a 3×3 matrix transforms 3D space. Let's see the classic examples:</p>
          
          <p><strong>Identity Matrix — "Do nothing"</strong></p>
<pre><code class="language-typescript">const I = [
  [1, 0],
  [0, 1]
];
// Multiplying any vector by I returns the same vector
// I × [x, y] = [x, y]</code></pre>
          
          <p><strong>Scaling Matrix — "Stretch or shrink"</strong></p>
<pre><code class="language-typescript">const scale = [
  [2, 0],   // Scale x by 2
  [0, 3]    // Scale y by 3
];
// [x, y] → [2x, 3y]</code></pre>
          
          <p><strong>Rotation Matrix — "Rotate space"</strong></p>
<pre><code class="language-typescript">// Rotate by angle θ counterclockwise
const rotate = (theta: number) => [
  [Math.cos(theta), -Math.sin(theta)],
  [Math.sin(theta),  Math.cos(theta)]
];
// This is worth memorising—it appears everywhere in graphics and physics</code></pre>
          
          <p><strong>Shear Matrix — "Skew space"</strong></p>
<pre><code class="language-typescript">const shearX = [
  [1, 0.5],  // x gets shifted by 0.5*y
  [0, 1]
];
// Turns rectangles into parallelograms</code></pre>
          
          <p>Your Three.js visualization will show these beautifully—watch how the entire coordinate grid warps when you apply each transformation. Every point moves simultaneously according to the same rule encoded in the matrix.</p>

          <hr>
          
          <h2>6. Why This Matters for ML/AI</h2>
          
          <p>Here's where everything clicks together for neural networks.</p>
          
          <p><strong>Neural network layers ARE matrix multiplications.</strong> A dense (fully connected) layer takes an input vector, multiplies it by a weight matrix, adds a bias, and applies an activation function:</p>
          
<pre><code class="language-typescript">// Single layer forward pass (simplified)
function denseLayer(input: Vector, weights: Matrix, bias: Vector): Vector {
  return activate(matmul(weights, input) + bias);
}</code></pre>
          
          <p><strong>Weight matrices encode learned relationships.</strong> Each entry W[i][j] represents "how much does input feature j influence output neuron i?" Training a neural network means adjusting these matrix entries.</p>
          
          <p><strong>Batch processing is matrix multiplication.</strong> Instead of processing one input at a time, stack N inputs as columns in a matrix. One matrix multiplication processes the entire batch:</p>
          
<pre><code class="language-typescript">// inputs: (features × batch_size)
// weights: (neurons × features)  
// outputs: (neurons × batch_size)  ← all N outputs computed at once!
const outputs = matmul(weights, inputs);</code></pre>
          
          <p>This is why GPUs matter—they're designed for exactly this kind of parallel matrix operation.</p>
          
          <p><strong>Understanding shapes prevents bugs.</strong> The most common error in deep learning code is dimension mismatch. If your layer expects (784,) inputs (flattened 28×28 images) but receives (28, 28), you'll get a shape error. Understanding how dimensions flow through matrix multiplications helps you debug instantly:</p>
          
<pre><code>Input: (batch_size, 784)
Layer 1 weights: (784, 256)  →  Output: (batch_size, 256)
Layer 2 weights: (256, 128)  →  Output: (batch_size, 128)
Layer 3 weights: (128, 10)   →  Output: (batch_size, 10)</code></pre>

          <hr>
          
          <h2>7. The Programming Perspective</h2>
          
          <p>In practice, you'll never write nested loops for matrix multiplication—libraries handle it efficiently.</p>
          
          <p><strong>NumPy:</strong></p>
<pre><code class="language-python">import numpy as np
C = np.matmul(A, B)   # Explicit
C = A @ B              # Python 3.5+ operator (preferred)</code></pre>
          
          <p><strong>TensorFlow/PyTorch:</strong></p>
<pre><code class="language-python">C = tf.matmul(A, B)   # TensorFlow
C = torch.mm(A, B)     # PyTorch (2D only)
C = torch.matmul(A, B) # PyTorch (handles batches)</code></pre>
          
          <p><strong>JavaScript (for your Three.js work):</strong></p>
<pre><code class="language-typescript">// Three.js Matrix4 class handles 4×4 transformations
const m = new THREE.Matrix4();
m.multiply(otherMatrix);  // In-place multiplication</code></pre>
          
          <p><strong>Broadcasting hint:</strong> Modern frameworks can handle "batch" dimensions automatically. A weight matrix (256, 128) multiplied with a batch (32, 256) broadcasts correctly to produce (32, 128). The rules can be subtle—consult documentation when shapes get complex.</p>

          <hr>
          
          <h2>8. Common Pitfalls</h2>
          
          <p><strong>Shape mismatch errors:</strong> The inner dimensions must match. If you're getting "shapes (X, Y) and (Z, W) not aligned," check that Y equals Z. Sketch out your expected shapes on paper.</p>
          
          <p><strong>Confusing @ with *:</strong> In NumPy, <code>A * B</code> is element-wise multiplication (same shape required), while <code>A @ B</code> is matrix multiplication. This catches everyone at least once.</p>
          
          <p><strong>Forgetting that order matters:</strong> <code>weights @ inputs</code> is not the same as <code>inputs @ weights</code>. Convention varies—some frameworks expect (batch, features), others (features, batch). Read the docs.</p>
          
          <p><strong>Transposition confusion:</strong> Sometimes you need to transpose a matrix (flip rows and columns) to make dimensions align. If A is (m, n), then A.T is (n, m). Common pattern: <code>X @ W.T</code> or <code>W.T @ X</code> depending on your convention.</p>
          
          <p><strong>Off-by-one with batch dimensions:</strong> Is your input shape (784,) or (1, 784) or (784, 1)? These are different: a 1D array versus a row vector versus a column vector. Frameworks sometimes reshape automatically, sometimes don't.</p>

          <hr>
          
          <h2>Wrapping Up</h2>
          
          <p>Matrix multiplication is deceptively simple in definition—row times column, sum the products—but unlocks extraordinary power. You've learned that matrices can represent transformations of space, that order matters because transformations don't commute, and that neural networks are fundamentally stacks of matrix multiplications learning useful transformations of data.</p>
          
          <p>In your Three.js visualization, watch how applying a matrix warps the entire grid. That warping is exactly what happens to data flowing through a neural network—each layer transforms the representation, bending and stretching the feature space until the final output separates cats from dogs, or predicts tomorrow's weather, or generates the next word in a sentence.</p>
          
          <p><em>Next up: we'll see how these ideas extend to tensors—matrices with more dimensions—and how backpropagation uses matrix calculus to learn those transformations.</em></p>

          </article>
        </div>

        <nav class="topic-nav">
          <a href="a1-vector-operations.html" class="nav-link prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">← A1: Vector Operations</span>
          </a>
          <a href="a3-matrix-transformations.html" class="nav-link next">
            <span class="nav-label">Next</span>
            <span class="nav-title">A3: Matrix Transformations →</span>
          </a>
        </nav>
      </div>

    </div>
  </main>

  <!-- Sticky Footer/Home Button -->
  <a href="../index.html" class="back-home" title="Back to Home">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
      <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/>
      <polyline points="9 22 9 12 15 12 15 22"/>
    </svg>
  </a>
</body>
</html>
