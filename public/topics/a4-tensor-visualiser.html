<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A4: Tensor Visualiser — Neural Pathways</title>
  <meta name="description" content="Show scalars to vectors to matrices to N-D tensors. Demonstrate reshaping, broadcasting, and slicing operations.">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <link rel="stylesheet" href="../css/style.css">

  <style>
    /* * Custom Layout Overrides for "Immersive Mode"
     * This creates the sticky visualization side-by-side layout
     */

    /* Base setup for the split layout container */
    .split-layout-container {
      width: 100%;
      max-width: 100%;
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    /* Default mobile/tablet view (Stacked) */
    .viz-panel {
      width: 100%;
      padding: 1rem;
    }
    
    .content-panel {
      width: 100%;
      padding: 1rem;
      max-width: 800px; /* Readable line length on mobile */
      margin: 0 auto;
    }

    /* The aspect ratio box */
    .viz-aspect-box {
      width: 100%;
      position: relative;
      /* Enforce 4:3 Aspect Ratio */
      aspect-ratio: 4 / 3; 
      background: #0f1115; /* Fallback dark bg */
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 10px 30px rgba(0,0,0,0.5);
      border: 1px solid rgba(255,255,255,0.1);
    }

    .viz-aspect-box iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: none;
    }

    /* Floating Home Button Styles */
    .back-home {
      position: fixed;
      bottom: 24px;
      right: 24px;
      width: 56px;
      height: 56px;
      background-color: #1a1d21; /* Dark surface color to match theme */
      color: #e2e8f0;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 20px rgba(0,0,0,0.4);
      z-index: 100;
      transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275);
      text-decoration: none; /* Remove underline */
    }
    
    .back-home:hover {
      background-color: #3b82f6; /* Primary blue accent */
      color: white;
      transform: translateY(-4px) scale(1.05);
      box-shadow: 0 8px 25px rgba(59, 130, 246, 0.4);
      border-color: transparent;
    }

    .back-home svg {
      width: 24px;
      height: 24px;
    }

    /* DESKTOP LAYOUT (Split Screen) */
    @media (min-width: 1024px) {
      .split-layout-container {
        display: flex;
        flex-direction: row;
        align-items: flex-start; /* Important for sticky to work */
        min-height: 100vh;
      }

      /* Left Side: Visualization (Sticky) */
      .viz-panel {
        /* Make it big: 60% of width */
        width: 60%;
        /* Sticky Magic */
        position: sticky;
        top: 0;
        height: 100vh; /* Fill vertical height of viewport */
        display: flex;
        align-items: center; /* Center viz vertically */
        justify-content: center;
        padding: 2rem;
        background: #050505; /* Slightly darker bg for viz area */
        border-right: 1px solid rgba(255,255,255,0.05);
        box-sizing: border-box;
        z-index: 10;
      }

      .viz-aspect-box {
        width: 100%; 
        /* The aspect ratio is handled by the class definition above */
      }

      /* Right Side: Content (Scrollable) */
      .content-panel {
        width: 40%;
        padding: 3rem;
        box-sizing: border-box;
        /* Add some bottom padding so content isn't hidden behind the fixed button */
        padding-bottom: 6rem;
      }

      /* Adjust breadcrumb/header spacing for this tight layout */
      .topic-header {
        margin-top: 1rem;
      }
    }

    /* Large desktop tweaks */
    @media (min-width: 1600px) {
      .viz-panel { width: 65%; }
      .content-panel { width: 35%; }
    }
  </style>
</head>
<body>
  <header class="header">
    <div class="container">
      <a href="../index.html" class="logo">
        <div class="logo-icon">
          <svg viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg">
            <circle cx="12" cy="12" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="12" cy="36" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="24" cy="18" r="4" fill="currentColor"/>
            <circle cx="24" cy="30" r="4" fill="currentColor"/>
            <circle cx="36" cy="24" r="5" fill="currentColor"/>
            <line x1="16" y1="12" x2="20" y2="18" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="16" y1="36" x2="20" y2="30" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="18" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="30" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
          </svg>
        </div>
        <div class="logo-text">
          <h1>Neural Pathways</h1>
          <span>Interactive ML Visualizations</span>
        </div>
      </a>
    </div>
  </header>

  <main class="topic-page">
    <div class="split-layout-container">
      
      <!-- 1. Visualization Panel (Sticky on Desktop) -->
      <aside class="viz-panel">
        <div class="viz-aspect-box">
          <iframe src="../../viz/A4-Tensor-Visualiser.html" title="Interactive Tensor Visualiser" loading="lazy"></iframe>
        </div>
      </aside>

      <!-- 2. Content Panel (Scrollable) -->
      <div class="content-panel">
        
        <!-- Navigation & Header moved inside the scrolling content pane -->
        <nav class="breadcrumb">
          <a href="../index.html">Home</a>
          <span>›</span>
          <a href="../index.html#cat-a">Linear Algebra</a>
          <span>›</span>
          <span>A4</span>
        </nav>

        <header class="topic-header" style="--category-color: var(--cat-a)">
          <div class="topic-meta">
            <span class="topic-badge" style="background: var(--cat-a)">A4</span>
            <span class="topic-category">Linear Algebra Fundamentals</span>
          </div>
          <h1>Tensor Visualiser</h1>
          <p class="topic-description">Show scalars → vectors → matrices → 3D+ tensors. Demonstrate reshaping, broadcasting, and slicing.</p>
        </header>

        <!-- Educational Content -->
        <div class="content-layout--single">
          <article class="educational-content">
          
          <p>You've mastered vectors (1D) and matrices (2D). Now it's time to remove the training wheels and meet the general concept that encompasses them both: the <strong>tensor</strong>. In deep learning, everything is a tensor. Your input data, your weights, your gradients, your outputs. Understanding tensors means understanding the fundamental data structure of modern AI.</p>
          
          <p>The good news: if you understand arrays in programming, you already understand tensors. They're just arrays that can have any number of dimensions.</p>

          <h2>1. What is a Tensor?</h2>
          
          <p>A tensor is a container for numbers arranged in a regular grid of any dimensionality. That's it. The term comes from physics and differential geometry where it has a more specific meaning, but in machine learning, "tensor" simply means "n-dimensional array."</p>
          
<pre><code class="language-typescript">// These are all tensors:
const scalar = 42;                          // 0D tensor
const vector = [1, 2, 3];                   // 1D tensor
const matrix = [[1, 2], [3, 4]];            // 2D tensor
const cube = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]];  // 3D tensor</code></pre>
          
          <p>The key insight is that vectors and matrices aren't special cases with their own rules. They're just tensors with 1 and 2 dimensions respectively. Once you understand tensors, you understand all of them simultaneously.</p>

          <hr>
          
          <h2>2. The Tensor Hierarchy</h2>
          
          <p>Let's climb the ladder from the simplest to the most complex, with real ML examples at each level.</p>
          
          <h3>Rank 0: Scalar (single number)</h3>
          
          <p>A scalar is a tensor with zero dimensions. It's just one number, but frameworks still treat it as a tensor for consistency.</p>
          
<pre><code class="language-python"># NumPy
loss = np.array(0.342)
print(loss.shape)  # ()  — empty tuple means 0 dimensions

# TensorFlow.js
const loss = tf.scalar(0.342);
console.log(loss.shape);  // []</code></pre>
          
          <p><em>ML examples:</em> Loss value, learning rate, accuracy metric, single prediction probability.</p>
          
          <h3>Rank 1: Vector (1D array)</h3>
          
          <p>A vector is a tensor with one dimension. We covered these in A1, but now you can see them as part of the bigger picture.</p>
          
<pre><code class="language-python"># NumPy
bias = np.array([0.1, -0.2, 0.3, 0.0])
print(bias.shape)  # (4,)

# TensorFlow.js
const bias = tf.tensor1d([0.1, -0.2, 0.3, 0.0]);
console.log(bias.shape);  // [4]</code></pre>
          
          <p><em>ML examples:</em> Bias vector in a layer, word embedding for a single word, 1D signal like audio amplitude over time, output probabilities for classification.</p>
          
          <h3>Rank 2: Matrix (2D array)</h3>
          
          <p>Matrices are tensors with two dimensions. You know these from A2 and A3.</p>
          
<pre><code class="language-python"># NumPy
weights = np.array([
    [0.1, 0.2, 0.3],
    [0.4, 0.5, 0.6]
])
print(weights.shape)  # (2, 3) — 2 rows, 3 columns

# TensorFlow.js
const weights = tf.tensor2d([
    [0.1, 0.2, 0.3],
    [0.4, 0.5, 0.6]
]);
console.log(weights.shape);  // [2, 3]</code></pre>
          
          <p><em>ML examples:</em> Weight matrix connecting layers, grayscale image, batch of 1D data points, attention scores, confusion matrix.</p>
          
          <h3>Rank 3: 3D Tensor</h3>
          
          <p>Here's where we go beyond familiar territory. A 3D tensor is like a stack of matrices, or a cube of numbers.</p>
          
<pre><code class="language-python"># NumPy — RGB image: height × width × channels
image = np.zeros((224, 224, 3))  # 224×224 pixels, 3 color channels
print(image.shape)  # (224, 224, 3)

# TensorFlow.js — sequence data
sequenceData = tf.zeros([32, 50, 256]);  # 32 sequences, 50 timesteps, 256 features
console.log(sequenceData.shape);  // [32, 50, 256]</code></pre>
          
          <p><em>ML examples:</em> Single RGB/color image (H×W×C), batch of grayscale images (batch×H×W), sequence data for RNNs (batch×timesteps×features), word embeddings for a sentence.</p>
          
          <h3>Rank 4+: Higher-Dimensional Tensors</h3>
          
          <p>For images in neural networks, we typically work with 4D tensors:</p>
          
<pre><code class="language-python"># NumPy — batch of RGB images
batch_of_images = np.zeros((32, 224, 224, 3))
# 32 images, each 224×224 pixels, each pixel has 3 color values
print(batch_of_images.shape)  # (32, 224, 224, 3)

# TensorFlow.js
const imageBatch = tf.zeros([32, 224, 224, 3]);
console.log(imageBatch.shape);  // [32, 224, 224, 3]</code></pre>
          
          <p><em>ML examples:</em> Batch of color images (batch×H×W×C), video data (batch×frames×H×W×C makes 5D), transformer attention (batch×heads×sequence×sequence).</p>

          <hr>
          
          <h2>3. Shape, Rank, and Size</h2>
          
          <p>Three numbers tell you everything about a tensor's structure:</p>
          
          <p><strong>Shape</strong> is a tuple listing the size of each dimension.</p>
          
<pre><code class="language-python">tensor = np.zeros((2, 3, 4))
print(tensor.shape)  # (2, 3, 4)</code></pre>
          
          <p>Read this as: "2 blocks, each containing 3 rows, each row having 4 elements."</p>
          
          <p><strong>Rank</strong> (or "ndim" in NumPy) is the number of dimensions. It's simply the length of the shape tuple.</p>
          
<pre><code class="language-python">print(tensor.ndim)  # 3
print(len(tensor.shape))  # 3 — same thing</code></pre>
          
          <p><strong>Size</strong> is the total count of numbers in the tensor. Multiply all shape dimensions together.</p>
          
<pre><code class="language-python">print(tensor.size)  # 2 × 3 × 4 = 24</code></pre>
          
          <p><strong>Why this matters:</strong> Neural network layers have strict expectations about input shapes. A Conv2D layer expects 4D input (batch, height, width, channels). A Dense layer expects 2D input (batch, features). Shape mismatches are the most common errors you'll encounter.</p>

          <hr>
          
          <h2>4. Reshaping: Same Data, Different Shape</h2>
          
          <p>Reshaping rearranges the structure of a tensor without changing its data. The total number of elements must stay the same.</p>
          
<pre><code class="language-python"># Start with a 2×6 matrix (12 elements)
original = np.array([
    [1, 2, 3, 4, 5, 6],
    [7, 8, 9, 10, 11, 12]
])

# Reshape to 3×4 (still 12 elements)
reshaped = original.reshape(3, 4)
# [[1, 2, 3, 4],
#  [5, 6, 7, 8],
#  [9, 10, 11, 12]]

# Reshape to 12×1 (column vector)
column = original.reshape(12, 1)

# Reshape to 1D (flatten)
flat = original.reshape(-1)  # -1 means "figure out this dimension"
# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</code></pre>
          
          <p><strong>Common reshapes in neural networks:</strong></p>
          
          <p><em>Flattening for dense layers:</em> After convolutional layers, you often need to flatten the 3D feature maps into a 1D vector for dense layers.</p>
          
<pre><code class="language-python"># After conv layers: (batch, 7, 7, 512)
# Need to feed to Dense layer: (batch, features)
x = x.reshape(batch_size, -1)  # becomes (batch, 7*7*512) = (batch, 25088)</code></pre>
          
          <p><em>Expanding dimensions:</em> Add a dimension of size 1 for broadcasting or to meet layer expectations.</p>
          
<pre><code class="language-python">vector = np.array([1, 2, 3])  # shape (3,)
row = vector.reshape(1, 3)    # shape (1, 3) — row vector
col = vector.reshape(3, 1)    # shape (3, 1) — column vector

# Or use expand_dims
row = np.expand_dims(vector, axis=0)  # adds dimension at position 0</code></pre>
          
          <p><em>Preparing images:</em> Single images need a batch dimension added before feeding to models.</p>
          
<pre><code class="language-python">image = load_image()  # shape (224, 224, 3)
batched = image.reshape(1, 224, 224, 3)  # shape (1, 224, 224, 3)
# Or: batched = np.expand_dims(image, axis=0)</code></pre>

          <hr>
          
          <h2>5. Broadcasting: Operations on Mismatched Shapes</h2>
          
          <p>Broadcasting is the magic that lets you add a scalar to a matrix, or multiply a matrix by a vector, without explicitly copying data. It's how frameworks handle operations between tensors of different shapes.</p>
          
          <p><strong>The core idea:</strong> When shapes don't match, smaller tensors are "virtually stretched" to match larger ones.</p>
          
<pre><code class="language-python"># Add scalar to matrix (scalar broadcasts to every element)
matrix = np.array([[1, 2], [3, 4]])
result = matrix + 10
# [[11, 12],
#  [13, 14]]

# Add vector to matrix (vector broadcasts across rows)
vector = np.array([100, 200])
result = matrix + vector
# [[101, 202],
#  [103, 204]]</code></pre>
          
          <p><strong>Broadcasting rules (simplified):</strong></p>
          <ol>
            <li>Compare shapes from right to left (trailing dimensions first)</li>
            <li>Dimensions are compatible if they're equal, or if one of them is 1</li>
            <li>Missing dimensions on the left are treated as size 1</li>
          </ol>
          
<pre><code>A: (4, 3)
B:    (3)  ← B is treated as (1, 3), then broadcast to (4, 3)
Result: (4, 3) ✓

A: (2, 3, 4)
B:       (4)  ← B is treated as (1, 1, 4), broadcast to (2, 3, 4)
Result: (2, 3, 4) ✓

A: (2, 3)
B: (4, 3)  ← 2 ≠ 4 and neither is 1
Result: Error! ✗</code></pre>
          
          <p><strong>Common broadcasting patterns in ML:</strong></p>
          
          <p><em>Bias addition:</em> Add a bias vector to every sample in a batch.</p>
          
<pre><code class="language-python"># batch of predictions: (32, 10)
# bias vector: (10,)
# bias broadcasts across the batch dimension
output = predictions + bias  # shape (32, 10)</code></pre>
          
          <p><em>Normalization:</em> Subtract mean and divide by std across features.</p>
          
<pre><code class="language-python"># data: (1000, 64) — 1000 samples, 64 features
mean = data.mean(axis=0)  # shape (64,)
std = data.std(axis=0)    # shape (64,)
normalized = (data - mean) / std  # broadcasts work automatically</code></pre>

          <hr>
          
          <h2>6. Slicing and Indexing</h2>
          
          <p>Extracting parts of tensors is essential for inspecting data, selecting samples, and implementing attention mechanisms.</p>
          
          <p><strong>Single element access:</strong></p>
          
<pre><code class="language-python">tensor = np.array([
    [[1, 2], [3, 4]],
    [[5, 6], [7, 8]]
])  # shape (2, 2, 2)

# Access element at position [1, 0, 1]
element = tensor[1, 0, 1]  # 6</code></pre>
          
          <p><strong>Slice notation:</strong> Use <code>start:stop</code> to grab ranges. Omit start or stop to mean "from beginning" or "to end."</p>
          
<pre><code class="language-python">matrix = np.array([
    [1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]
])

# First two rows
matrix[:2]      # [[1,2,3,4], [5,6,7,8]]

# Last column
matrix[:, -1]   # [4, 8, 12]

# Top-left 2×2 block
matrix[:2, :2]  # [[1,2], [5,6]]

# Every other row
matrix[::2]     # [[1,2,3,4], [9,10,11,12]]</code></pre>
          
          <p><strong>ML-relevant slicing examples:</strong></p>
          
<pre><code class="language-python"># Get a single sample from a batch
batch = np.zeros((32, 224, 224, 3))
single_image = batch[0]  # shape (224, 224, 3)

# Get the red channel only
red_channel = batch[:, :, :, 0]  # shape (32, 224, 224)

# Get predictions for class 0 across batch
predictions = np.zeros((32, 10))  # 32 samples, 10 classes
class_0_probs = predictions[:, 0]  # shape (32,)

# Sequence slicing for transformers
sequence = np.zeros((32, 512, 768))  # batch, seq_len, hidden
first_token = sequence[:, 0, :]  # CLS token: shape (32, 768)</code></pre>

          <hr>
          
          <h2>7. Why This Matters for ML/AI</h2>
          
          <p><strong>All neural network data is tensors.</strong> There's no escaping them. Input images, text embeddings, weight matrices, gradients during backprop, attention scores. Everything flows through your network as tensors.</p>
          
          <p><strong>Shape errors are the #1 debugging challenge.</strong> You'll see errors like "Expected shape (None, 784) but got (32, 28, 28)" constantly. Understanding tensor shapes lets you trace through your network mentally and spot where dimensions go wrong.</p>
          
<pre><code class="language-python"># Common debugging pattern
def debug_shapes(model, sample_input):
    x = sample_input
    print(f"Input: {x.shape}")
    for i, layer in enumerate(model.layers):
        x = layer(x)
        print(f"After layer {i} ({layer.name}): {x.shape}")</code></pre>
          
          <p><strong>Broadcasting prevents bugs (and causes them).</strong> Broadcasting is powerful but can silently do the wrong thing if you're not careful. A shape (64, 1) tensor and a shape (1, 64) tensor will broadcast to (64, 64) when multiplied. Is that what you wanted? Understanding broadcasting helps you write correct code and catch bugs.</p>
          
          <p><strong>Reshaping connects layer types.</strong> Convolutional layers output 4D tensors. Dense layers expect 2D. Reshape (flatten) bridges them. Transformer attention reshapes constantly (splitting heads, merging heads). Know your reshapes.</p>
          
          <p><strong>Efficient batching requires tensor thinking.</strong> Processing one sample at a time is slow. Batching multiple samples into a single tensor lets the GPU parallelize. But this means every operation must handle the batch dimension correctly.</p>
          
<pre><code class="language-python"># Slow: loop over samples
for sample in samples:
    output = model(sample)

# Fast: batch everything
outputs = model(samples_tensor)  # GPU processes all at once</code></pre>

          <hr>
          
          <h2>Wrapping Up</h2>
          
          <p>Tensors are the universal language of deep learning. They generalize the vectors and matrices you already know to arbitrary dimensions, and the operations you've learned (multiplication, addition, transformation) extend naturally.</p>
          
          <p>The key concepts to internalize: shape tells you structure, reshaping rearranges without changing data, broadcasting handles mismatched shapes automatically, and slicing extracts the pieces you need.</p>
          
          <p>In your Three.js visualization, watch how data flows through tensor operations. See how a 4D batch of images gets reshaped, convolved, and flattened as it moves through a CNN. That visual intuition will make debugging shape errors almost trivial.</p>
          
          <p><em>Next, we'll explore the eigenvalue decomposition, which reveals the fundamental "directions" that a matrix transformation prefers.</em></p>

          </article>
        </div>

        <nav class="topic-nav">
          <a href="a3-matrix-transformations.html" class="nav-link prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">← A3: Matrix Transformations</span>
          </a>
          <a href="a5-eigenvalues-eigenvectors.html" class="nav-link next">
            <span class="nav-label">Next</span>
            <span class="nav-title">A5: Eigenvalues & Eigenvectors →</span>
          </a>
        </nav>
      </div>

    </div>
  </main>

  <!-- Sticky Footer/Home Button -->
  <a href="../index.html" class="back-home" title="Back to Home">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
      <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/>
      <polyline points="9 22 9 12 15 12 15 22"/>
    </svg>
  </a>
</body>
</html>
