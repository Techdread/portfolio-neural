<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>C6: Softmax and Cross-Entropy — Neural Pathways</title>
  <meta name="description" content="From raw scores to probabilities to loss: understand the complete classification pipeline.">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">

  <style>
    .split-layout-container {
      width: 100%;
      max-width: 100%;
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    .viz-panel {
      width: 100%;
      padding: 1rem;
    }
    
    .content-panel {
      width: 100%;
      padding: 1rem;
      max-width: 800px;
      margin: 0 auto;
    }

    .viz-aspect-box {
      width: 100%;
      position: relative;
      aspect-ratio: 4 / 3; 
      background: #0f1115;
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 10px 30px rgba(0,0,0,0.5);
      border: 1px solid rgba(255,255,255,0.1);
    }

    .viz-aspect-box iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border: none;
    }

    .back-home {
      position: fixed;
      bottom: 24px;
      right: 24px;
      width: 56px;
      height: 56px;
      background-color: #1a1d21;
      color: #e2e8f0;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 20px rgba(0,0,0,0.4);
      z-index: 100;
      transition: all 0.2s cubic-bezier(0.175, 0.885, 0.32, 1.275);
      text-decoration: none;
    }
    
    .back-home:hover {
      background-color: #3b82f6;
      color: white;
      transform: translateY(-4px) scale(1.05);
      box-shadow: 0 8px 25px rgba(59, 130, 246, 0.4);
      border-color: transparent;
    }

    .back-home svg {
      width: 24px;
      height: 24px;
    }

    @media (min-width: 1024px) {
      .split-layout-container {
        display: flex;
        flex-direction: row;
        align-items: flex-start;
        min-height: 100vh;
      }

      .viz-panel {
        width: 60%;
        position: sticky;
        top: 0;
        height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        padding: 2rem;
        background: #050505;
        border-right: 1px solid rgba(255,255,255,0.05);
        box-sizing: border-box;
        z-index: 10;
      }

      .viz-aspect-box {
        width: 100%; 
      }

      .content-panel {
        width: 40%;
        padding: 3rem;
        box-sizing: border-box;
        padding-bottom: 6rem;
      }

      .topic-header {
        margin-top: 1rem;
      }
    }

    @media (min-width: 1600px) {
      .viz-panel { width: 65%; }
      .content-panel { width: 35%; }
    }
  </style>
</head>
<body>
  <header class="header">
    <div class="container">
      <a href="../index.html" class="logo">
        <div class="logo-icon">
          <svg viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg">
            <circle cx="12" cy="12" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="12" cy="36" r="4" fill="currentColor" opacity="0.8"/>
            <circle cx="24" cy="18" r="4" fill="currentColor"/>
            <circle cx="24" cy="30" r="4" fill="currentColor"/>
            <circle cx="36" cy="24" r="5" fill="currentColor"/>
            <line x1="16" y1="12" x2="20" y2="18" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="16" y1="36" x2="20" y2="30" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="18" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
            <line x1="28" y1="30" x2="32" y2="24" stroke="currentColor" stroke-width="1.5" opacity="0.6"/>
          </svg>
        </div>
        <div class="logo-text">
          <h1>Neural Pathways</h1>
          <span>Interactive ML Visualizations</span>
        </div>
      </a>
    </div>
  </header>

  <main class="topic-page">
    <div class="split-layout-container">
      
      <aside class="viz-panel">
        <div class="viz-aspect-box">
          <iframe src="../viz/C6-softmax-cross-entropy-visualization.html" title="Softmax and Cross-Entropy Visualization" loading="lazy"></iframe>
        </div>
      </aside>

      <div class="content-panel">
        
        <nav class="breadcrumb">
          <a href="../index.html">Home</a>
          <span>›</span>
          <a href="../index.html#cat-c">Neural Network Ops</a>
          <span>›</span>
          <span>C6</span>
        </nav>

        <header class="topic-header" style="--category-color: var(--cat-c)">
          <div class="topic-meta">
            <span class="topic-badge" style="background: var(--cat-c)">C6</span>
            <span class="topic-category">Neural Network Operations</span>
          </div>
          <h1>Softmax and Cross-Entropy</h1>
          <p class="topic-description">From raw scores to probabilities to loss: the complete classification pipeline.</p>
        </header>

        <div class="content-layout--single">
          <article class="educational-content">
            
            <section class="content-section">
              <h2>The Classification Pipeline</h2>
              <p>When a neural network classifies an image as "cat," "dog," or "bird," what actually happens inside? The network doesn't output the word "cat." It outputs numbers—raw scores for each possible class. These scores need to become probabilities, and those probabilities need to become a single number measuring how wrong the prediction was.</p>
              <div class="formula-box">
                Raw Scores (Logits) → Softmax → Probabilities → Cross-Entropy → Loss<br>
                [2.0, 1.0, 0.1] → [0.659, 0.242, 0.099] → 0.42
              </div>
              <p>Each step has a purpose. Softmax turns arbitrary numbers into probabilities. Cross-entropy measures how far those probabilities are from the truth. Together, they form the standard ending for classification networks.</p>
            </section>

            <section class="content-section">
              <h2>What Are Logits?</h2>
              <p>The last layer of a classification network produces <strong>logits</strong>—raw, unnormalized scores. One score per class.</p>
              <pre><code>Network output for 3-class problem:
  Cat:  2.0
  Dog:  1.0
  Bird: 0.1</code></pre>
              <p>These numbers have no restrictions: they can be positive, negative, or zero, any magnitude, and they don't sum to anything special.</p>
              <p>Logits encode the network's relative preferences. A higher score means the network favors that class. But logits aren't probabilities—we need to convert them.</p>
            </section>

            <section class="content-section">
              <h2>The Softmax Function</h2>
              <p>Softmax transforms logits into probabilities:</p>
              <div class="formula-box">softmax(zᵢ) = e^zᵢ / Σⱼ e^zⱼ</div>
              <p>In words: take e raised to each score, then normalize by dividing by the sum of all exponentials.</p>
              
              <h3>Step-by-step example:</h3>
              <p>Starting with logits [2.0, 1.0, 0.1]:</p>
              <p><strong>Step 1: Compute exponentials</strong></p>
              <pre><code>e^2.0 = 7.389
e^1.0 = 2.718
e^0.1 = 1.105</code></pre>
              <p><strong>Step 2: Sum the exponentials</strong></p>
              <pre><code>7.389 + 2.718 + 1.105 = 11.212</code></pre>
              <p><strong>Step 3: Divide each by the sum</strong></p>
              <pre><code>Cat:  7.389 / 11.212 = 0.659  (65.9%)
Dog:  2.718 / 11.212 = 0.242  (24.2%)
Bird: 1.105 / 11.212 = 0.099  (9.9%)</code></pre>
              <p><strong>Verify:</strong> 0.659 + 0.242 + 0.099 = 1.000 ✓</p>
            </section>

            <section class="content-section">
              <h2>Why Exponentials?</h2>
              <p>The exponential function has special properties that make it perfect for this job:</p>
              <ul>
                <li><strong>Always positive:</strong> e^x > 0 for any x. This guarantees all probabilities are positive.</li>
                <li><strong>Preserves ordering:</strong> If z₁ > z₂, then e^z₁ > e^z₂. The class with the highest logit gets the highest probability.</li>
                <li><strong>Amplifies differences:</strong> The exponential grows rapidly. Small differences in logits become larger differences in probabilities.</li>
              </ul>
              <p>When the network is confident (large differences in logits), softmax produces sharp probability distributions. When uncertain (similar logits), it produces softer distributions.</p>
            </section>

            <section class="content-section">
              <h2>Temperature: Controlling Confidence</h2>
              <p>Softmax has an optional parameter called temperature (T):</p>
              <div class="formula-box">softmax(zᵢ; T) = e^(zᵢ/T) / Σⱼ e^(zⱼ/T)</div>
              <ul>
                <li><strong>Low temperature (T = 0.5):</strong> Distribution becomes sharper—more confident</li>
                <li><strong>High temperature (T = 2.0):</strong> Distribution becomes softer—less confident</li>
                <li><strong>T → 0:</strong> Approaches argmax. One class gets probability 1</li>
                <li><strong>T → ∞:</strong> Approaches uniform distribution</li>
              </ul>
              <p>Temperature is used during inference to control randomness in text generation and other sampling tasks.</p>
            </section>

            <section class="content-section">
              <h2>Cross-Entropy Loss</h2>
              <p>Now we have probabilities. How do we measure how wrong they are?</p>
              <p><strong>Cross-entropy loss</strong> compares the predicted probability distribution to the true distribution:</p>
              <div class="formula-box">L = -Σᵢ yᵢ · log(pᵢ)</div>
              <p>Where yᵢ is the true label (1 for correct class, 0 for others) and pᵢ is the predicted probability.</p>
              <p>For classification with one-hot labels, this simplifies: <strong>only the correct class matters!</strong></p>
              <div class="formula-box">L = -log(p_correct)</div>
              <p>Cross-entropy loss is simply the negative log of the probability assigned to the true class.</p>
            </section>

            <section class="content-section">
              <h2>The Log Penalty</h2>
              <p>The negative log function creates a harsh penalty structure:</p>
              <table>
                <thead>
                  <tr><th>p_correct</th><th>Loss</th></tr>
                </thead>
                <tbody>
                  <tr><td>0.99</td><td>0.01 (tiny)</td></tr>
                  <tr><td>0.90</td><td>0.11</td></tr>
                  <tr><td>0.50</td><td>0.69</td></tr>
                  <tr><td>0.10</td><td>2.30</td></tr>
                  <tr><td>0.01</td><td>4.61 (huge!)</td></tr>
                </tbody>
              </table>
              <p>The pattern is clear:</p>
              <ul>
                <li><strong>Confident and correct:</strong> Probability near 1 → loss near 0</li>
                <li><strong>Uncertain:</strong> Probability around 0.5 → moderate loss</li>
                <li><strong>Confident and wrong:</strong> Probability near 0 → loss explodes</li>
              </ul>
              <p>This asymmetry is the genius of cross-entropy. It <em>severely punishes</em> confident wrong predictions.</p>
            </section>

            <section class="content-section">
              <h2>Numerical Stability</h2>
              <p>There's a practical problem with naive softmax computation:</p>
              <ul>
                <li><strong>Overflow:</strong> If logits are large, e^z explodes (e^1000 = Infinity)</li>
                <li><strong>Underflow:</strong> If logits are very negative, e^z becomes zero</li>
              </ul>
              <p><strong>Solution: Subtract the maximum</strong></p>
              <p>A beautiful property of softmax: subtracting a constant from all logits doesn't change the result. By choosing c = max(z), we ensure the largest exponent is e^0 = 1, preventing overflow.</p>
              <p>This is the <strong>log-sum-exp trick</strong>, and every deep learning framework uses it automatically.</p>
            </section>

            <section class="content-section">
              <h2>The Elegant Gradient</h2>
              <p>When training, we need the gradient of the loss with respect to logits. The combined softmax + cross-entropy has an elegant gradient:</p>
              <div class="formula-box">∂L/∂zᵢ = pᵢ - yᵢ</div>
              <p>That's it! The gradient is simply the predicted probability minus the true label.</p>
              <ul>
                <li><strong>For the correct class:</strong> If p_correct = 0.8, gradient is -0.2. Negative → increase this logit.</li>
                <li><strong>For incorrect classes:</strong> If p_wrong = 0.15, gradient is 0.15. Positive → decrease this logit.</li>
              </ul>
              <p>The network learns to increase logits for correct classes and decrease logits for incorrect ones.</p>
            </section>

            <section class="content-section highlight-section">
              <h2>Why This Matters for ML/AI</h2>
              <ul>
                <li><strong>Every classification network uses this:</strong> Image, text, token classification—softmax + cross-entropy is the universal ending.</li>
                <li><strong>Debugging training issues:</strong> NaN loss often means log(0); infinite loss means overflow in exponentials.</li>
                <li><strong>Model calibration:</strong> Cross-entropy encourages calibrated predictions that reflect true frequencies.</li>
                <li><strong>Temperature tuning:</strong> For generation tasks, temperature controls diversity.</li>
              </ul>
            </section>

            <section class="content-section">
              <h2>Key Takeaways</h2>
              <ol>
                <li><strong>Logits are raw scores</strong> with no restrictions—what the network actually outputs</li>
                <li><strong>Softmax converts logits to probabilities</strong> using exponentials and normalization</li>
                <li><strong>Exponentials amplify differences</strong>, making confident predictions sharper</li>
                <li><strong>Cross-entropy loss = -log(p_correct)</strong>—only the true class matters</li>
                <li><strong>The log penalty is harsh:</strong> confident wrong predictions are severely punished</li>
                <li><strong>Numerical stability matters:</strong> use the stable implementation from your framework</li>
                <li><strong>The gradient is beautiful:</strong> ∂L/∂z = p - y (predicted minus true)</li>
              </ol>
            </section>

          </article>
        </div>

        <nav class="topic-nav">
          <a href="c5-loss-functions.html" class="nav-link prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">← C5: Loss Functions</span>
          </a>
          <a href="d1-gradient-descent-variants.html" class="nav-link next">
            <span class="nav-label">Next</span>
            <span class="nav-title">D1: Gradient Descent Variants →</span>
          </a>
        </nav>
      </div>

    </div>
  </main>

  <a href="../index.html" class="back-home" title="Back to Home">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
      <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/>
      <polyline points="9 22 9 12 15 12 15 22"/>
    </svg>
  </a>
</body>
</html>
